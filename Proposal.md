# Global Catastrophic Risk

**Final Project Proposal**

Will Borom, Kyle Dotterrer, Jeff Moore, Abby Vorhaus

_29 March, 2021_

### Proposal

Global catastrophic risk is a hypothetical future event which could damage human well-being on a global scale [3]. This concept may seem a far-off fantasy to be relegated to the pages of science fiction novels, but in reality it may constitute one of the most pressing issues the human race currently faces. Some estimates place the likelihood of human extinction as a result of global catastrophic risks as high as twenty percent by the year 2100 [4]. Despite this gloomy forecast, most sources of risk, including those that represent the greatest probabilities of undesirable outcomes, receive little attention in the public sphere. We intend to address the lack of visibility of this ostensibly important set of issues in our project. We will construct a narrative application that highlights the most salient of these risks in a format that is both informative and engaging. Our goal is to elicit both an intellectual and emotional response from the reader, compelling them to further explore the sources of risk we present, our current degree of exposure, and potential mitigation strategies. We believe such public awareness is the first step towards diminishing the possibility of stumbling into our own self-destruction.

Our narrative application will explore three distinct sources of global catastrophic risk. The sources of risk we choose to highlight are machine superintelligence, nuclear weapons, and resource depletion and ecological destruction. We choose these three sources of risk for a number of practical reasons. First, they vary in their degree of public exposure. The issue of ecological destruction is relatively well-known thanks to a collection of highly-visible popular media (documentary films, dramatizations, bestselling nonfiction works, etc.) that target the subject [1, 2]. In contrast, the existential risk posed by machine superintelligence is currently nothing more than a mildly-interesting science-fiction thought experiment for most individuals. The Terminator franchise is likely the closest most people come to considering the prospect of machine superintelligence, yet most experts agree the plot depicts a scenario that departs widely from those that are most likely to occur and about which we should be most concerned. Furthermore, these sources of risk provide opportunities for both “traditional” data collection and visualization methods as well as novel analysis and visualization. On issues related to ecological destruction and nuclear weaponry we foresee encountering a relatively high availability of data for use in analysis and visualization. For issues relating to machine superintelligence, however, established and well-structured datasets are largely nonexistent, requiring that we perform some level of data collection ourselves to address this shortcoming. Finally, and perhaps most importantly, each of us finds at least one of these sources of risk personally compelling and deserving of greater public exposure.

For each of these three sources of risk, we will present a narrative analysis along three distinct dimensions. While many non-anthropogenic sources of global catastrophic risk exist, the set of risks identified above should indicate that we choose to focus solely on those that are widely-considered man-made. Accordingly, we will focus our narrative on how humanity’s rapid development of technology, while productive and necessary for improving our collective condition, has had the side-effect of increasing the likelihood that we experience unintended self-destruction. A major component of our narrative will address this change over time with respect to both public opinion and public policy to help the reader understand the dynamic nature of this topic. Including a specific focus on the current shortcomings of perception and policy should impress upon the reader the need for action - a course correction - with respect to these issues.

Turning our attention to the inner workings of the project we are about to undertake, we would like to acknowledge the risks and costs. Perhaps the biggest risk our team will face in this project will be a lack of viable data. While this is unlikely to plague the area of ecological destruction or nuclear weapons, significantly less data exists on the topics of machine superintelligence and weaponized nanotechnology due to the smaller exposure and relative novelty of these topics. Our team anticipates the use of more creative approaches to overcome this obstacle and generate data. One such approach may be to collect academic papers and perform text analysis to assess how often cutting edge papers address some of the more nefarious implications of their work if they even do. A secondary risk but one that drives how we approach these topics is a failure to be taken seriously. These topics are existential risks, but those who discuss them, our team included, must walk a fine line in their presentation. Else, we risk being written off as doomsday prophets relegated to sit among the conspiracy theorists of the world. We would deem this  failure of our project because it would lack the capacity to reach and inform a wider audience. For costs, we do not anticipate any monetary costs as the intention of the team is to utilize only open source data and software. However, we do expect a significant time cost especially during the data collection and preprocessing stages. Even on the topics with ample data, in order to accurately portray our narrative it will be necessary to collect data from numerous sources. Condensing and preparing this data for integration will likely take the bulk of the time on this project which extends from now until the end of the spring semester. 

This is an ambitious project to undertake with a wide scope. However, we feel that our team is capable of achieving the goals put forward in this proposal. We feel strongly that this topic needs to be addressed to a much greater degree than it currently is, and we hope to initiate that wider degree of investigation in this project.  

### References

1. Gates, Bill. How to Avoid a Climate Disaster. 2021.
2. Guggenheim, Davis. An Inconvenient Truth. 2006.
3. Bostrom, Nick (2008). Global Catastrophic Risks (PDF). Oxford University Press. p. 1.
4. Sandberg, Anders and Bostrom, Nick. Global Catastrophic Risks Survey. FHI Technical Report. (2008).
